test


#m-provider.tf :프로바이더 생성


provider "aws" {
  #version = "~> 1.33"
  region  = "us-east-1"
}

provider "aws" {
  alias   = "califonia"   
  region  = "us-west-1"
}

#variable :변수생성
variable "amazon_linux_west" {
# Amazon Linux AMI 2017.03.1 (HVM), SSD Volume Type - ami-4af5022c
    default = "ami-04e59c05167ea7bd5"
}

variable "amazon_linux_east" {
# Amazon Linux AMI 2017.03.1 (HVM), SSD Volume Type - ami-4af5022c
    default = "ami-09d95fab7fff3776c"
}
variable "dev_keyname" {
    default = "Hholic"
}
variable "alb_account_id_east" {
    default = "127311923021"
}
variable "alb_account_id_west" {
    default = "027434742980"
}

#key.tf :키생성
resource "aws_key_pair" "Hholic-sshkey" {
  key_name   = "Hholic"
  public_key = "ssh-rsa ~~~~ ec2-user@ip-192-168-1-115"
}



#VPC.tf VPC생성
resource "aws_vpc" "east-vpc" {
cidr_block = "10.0.0.0/16"
enable_dns_hostnames = true
enable_dns_support = true
instance_tenancy = "default"
tags = {
Name = "east-vpc"
}
}

resource "aws_vpc" "west-vpc" {
provider = aws.califonia
cidr_block = "20.0.0.0/16"
enable_dns_hostnames = true
enable_dns_support = true
instance_tenancy = "default"
tags = {
Name = "west-vpc"
}
}

#Subnet.tf : 서브넷 생성
resource "aws_subnet" "east-1-public_1a" {
vpc_id = aws_vpc.east-vpc.id
availability_zone = "us-east-1a"
cidr_block = "10.0.1.0/24"
tags = {
Name = "east-1-public-1a"
}
}

resource "aws_subnet" "east-1-public_1c" {
vpc_id = aws_vpc.east-vpc.id
availability_zone = "us-east-1c"
cidr_block = "10.0.2.0/24"
tags = {
Name = "east-1-public-1c"
}
}
h
resource "aws_subnet" "west-1-public_1a" {
provider = aws.califonia
vpc_id = aws_vpc.west-vpc.id
availability_zone = "us-west-1a"
cidr_block = "20.0.1.0/24"
tags = {
Name = "west-1st-1-public-1a"
}
}

resource "aws_subnet" "west-1-public_1c" {
provider = aws.califonia
vpc_id = aws_vpc.west-vpc.id
availability_zone = "us-west-1c"
cidr_block = "20.0.2.0/24"
tags = {
Name = "west-1st-1-public-1c"
}
}


#IGW.tf : 인터넷게이트웨이 생성

resource "aws_internet_gateway" "esat-1-IGW" {
vpc_id = aws_vpc.east-vpc.id
tags = {
Name = "esat-1-IGW"
}
}

resource "aws_internet_gateway" "west-1-IGW" {
provider =aws.califonia
vpc_id = aws_vpc.west-vpc.id
tags = {
Name = "west-1-IGW"
}
}


#RouteTable.tf : 기본 라우팅 테이블 생성

resource "aws_route_table" "rt-east-1-public"{
    vpc_id= aws_vpc.east-vpc.id
        route {
            cidr_block= "0.0.0.0/0"
            gateway_id= aws_internet_gateway.esat-1-IGW.id
              }
    tags = {
    Name = "rt-east-1-public"
        }
}

resource "aws_route_table_association" "rt-east-1_public_1a"{
subnet_id= aws_subnet.east-1-public_1a.id
route_table_id=aws_route_table.rt-east-1-public.id
}

resource "aws_route_table_association" "rt-east-1_public_1c"{
subnet_id= aws_subnet.east-1-public_1c.id
route_table_id= aws_route_table.rt-east-1-public.id
}



resource "aws_route_table" "rt-west-1-public"{
    provider =aws.califonia
    vpc_id= aws_vpc.west-vpc.id
        route {
            cidr_block= "0.0.0.0/0"
            gateway_id= aws_internet_gateway.west-1-IGW.id
              }
    tags = {
    Name = "rt-west-1-public"
        }
}

resource "aws_route_table_association" "rt-west-1_public_1a"{
provider =aws.califonia
subnet_id= aws_subnet.west-1-public_1a.id
route_table_id=aws_route_table.rt-west-1-public.id
}

resource "aws_route_table_association" "rt-west-1_public_1c"{
provider =aws.califonia
subnet_id= aws_subnet.west-1-public_1c.id
route_table_id= aws_route_table.rt-west-1-public.id
}


#DFT-SG 기본 시큐리티 그룹 생성

resource "aws_default_security_group" "esat-vpc_SG_DFT" {
    vpc_id= aws_vpc.east-vpc.id
        ingress {
            protocol = -1
            self = true
            from_port = 0
            to_port = 0
            }
        egress {
            from_port = 0
            to_port = 0
            protocol = "-1"
            cidr_blocks = ["0.0.0.0/0"]
            }
    tags = {
        Name = "esat-vpc_SG_DFT"
            }
        }

resource "aws_default_security_group" "west-vpc_SG_DFT" {
    provider =aws.califonia
    vpc_id= aws_vpc.west-vpc.id
        ingress {
            protocol = -1
            self = true
            from_port = 0
            to_port = 0
            }
        egress {
            from_port = 0
            to_port = 0
            protocol = "-1"
            cidr_blocks = ["0.0.0.0/0"]
            }
    tags = {
        Name = "west-vpc_SG_DFT"
            }
        }


#alb-SG :어플리케이션 로드발란서용 시큐리티 그룹 생성
resource "aws_security_group" "east-alb-SG" {
    name = "esat-alb-SG"
    description = "open HTTP port for ALB"
    vpc_id = "${aws_vpc.east-vpc.id}"
    #vpc_id = "${aws_vpc.user05_vpc_A.id}"
        ingress {
            from_port = 80
            to_port = 80
            protocol = "tcp"
            cidr_blocks = ["0.0.0.0/0"]
                }
          ingress {
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }
        egress {
            from_port = 0
            to_port = 0
            protocol = "-1"
            cidr_blocks = ["0.0.0.0/0"]
                }
    tags = {
        Name = "east-alb-SG"
            }
}

resource "aws_security_group" "west-alb-SG" {
    provider = aws.califonia
    name = "west-alb-SG"
    description = "open HTTP port for ALB"
    vpc_id = "${aws_vpc.west-vpc.id}"

        ingress {
            from_port = 80
            to_port = 80
            protocol = "tcp"
            cidr_blocks = ["0.0.0.0/0"]
                }
          ingress {
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }
        egress {
            from_port = 0
            to_port = 0
            protocol = "-1"
            cidr_blocks = ["0.0.0.0/0"]
                }
    tags = {
        Name = "west-alb-SG"
            }
}





#vpc-pc :peer링커낵션 생성 및 라우팅 추가



data "aws_caller_identity" "west-peer" {
  provider = aws.califonia
}

# Requester's side of the connection.
resource "aws_vpc_peering_connection" "west-peer" {
  vpc_id        = aws_vpc.east-vpc.id
  peer_vpc_id   = aws_vpc.west-vpc.id
  peer_owner_id = data.aws_caller_identity.west-peer.account_id
  peer_region   = "us-west-1"
  auto_accept   = false

  tags = {
    Side = "Requester"
  }
}

# Accepter's side of the connection.
resource "aws_vpc_peering_connection_accepter" "west-peer" {
  provider                  = aws.califonia
  vpc_peering_connection_id = aws_vpc_peering_connection.west-peer.id
  auto_accept               = true

  tags = {
    Side = "Accepter"
  }
}



resource "aws_route" "peer-east-rt" {
  route_table_id            = aws_route_table.rt-east-1-public.id
  destination_cidr_block    = aws_vpc.west-vpc.cidr_block
  vpc_peering_connection_id = aws_vpc_peering_connection.west-peer.id
}

resource "aws_route" "peer-west-rt" {
  provider = aws.califonia
  route_table_id            = aws_route_table.rt-west-1-public.id
  destination_cidr_block    = aws_vpc.east-vpc.cidr_block
  vpc_peering_connection_id = aws_vpc_peering_connection.west-peer.id
}



#createS3 :alb 로그를 위한 S3 버킷 생성
resource "aws_s3_bucket" "hholic-alb-s3" {
    bucket = "hholic-alb-log.com"
    policy = <<EOF
{
        "Version": "2012-10-17",
        "Statement": [
        {
        "Effect": "Allow",
        "Principal": {
            "AWS": "arn:aws:iam::${var.alb_account_id_east}:root","AWS": "arn:aws:iam::${var.alb_account_id_west}:root"
                    },
        "Action": "s3:PutObject",
        "Resource": "arn:aws:s3:::hholic-alb-log.com/*"
        }
    ]
}
EOF
lifecycle_rule {
id = "log_lifecycle"
prefix = ""
enabled = true
transition {
days = 30
storage_class = "GLACIER"
}
expiration {
days = 90
}
}
lifecycle {
prevent_destroy = false
}
}






#alb-TG.tf 

resource "aws_alb_target_group" "hholic-east-frontend" {
name = "hholic-east-frontend-TG"
port = 80
protocol = "HTTP"
vpc_id = "${aws_vpc.east-vpc.id}"
health_check {
interval = 30
path = "/"
healthy_threshold = 3
unhealthy_threshold = 3
}
tags = { Name = "Frontend Target Group" }
}

#resource "aws_alb_target_group_attachment" "frontend" {
#target_group_arn = "${aws_alb_target_group.user05-frontend.arn}"
#target_id = "${aws_instance.user05_ec2_1a.id}"
#port = 80
#}
#resource "aws_alb_target_group_attachment" "frontend_1c" {
#target_group_arn = "${aws_alb_target_group.user05-frontend.arn}"
#target_id = "${aws_instance.user05_EC2_1c.id}"
#port = 80
#}




#alb-listener.tf

resource "aws_alb_listener" "hholic-east-http" {
load_balancer_arn = "${aws_alb.hholic-east-alb.arn}"
port = "80"
protocol = "HTTP"
default_action {
target_group_arn = "${aws_alb_target_group.hholic-east-frontend.arn}"
type = "forward"
}
}



#autoscale-alb.tf
resource "aws_alb" "hholic-east-alb" {
    name = "hholic-east-alb-autoscaling"
    internal = false
    security_groups = ["${aws_security_group.east-alb-SG.id}"]
    subnets = [
       "${aws_subnet.east-1-public_1a.id}",
       "${aws_subnet.east-1-public_1c.id}"
    ]
    access_logs {
        bucket = "${aws_s3_bucket.hholic-alb-s3.id}"
        prefix = "frontend-alb"
        enabled = true
    }
    tags = {
        Name = "hholic-east--alb-autoscaling"
    }
    lifecycle { create_before_destroy = true }
}



#alb-launch.tf

resource "aws_launch_configuration" "east-web" {
  name_prefix = "east-autoscaling-web"

  image_id = "${var.amazon_linux}"
  instance_type = "t2.nano"
  key_name = "Hholic"
  security_groups = [
    "${aws_security_group.east-alb-SG.id}",
    "${aws_default_security_group.esat-vpc_SG_DFT.id}",
  ]
  associate_public_ip_address = true
    
  user_data = <<USER_DATA
#!/bin/bash
yum update
yum -y install httpd
echo "<html>" > /var/www/html/index.html
echo "Hello" >> /var/www/html/index.html
echo "<img src=\"CloudFront URL\"> >> /var/www/html/index.html"
echo "</html>" >> /var/www/html/index.html
systemctl start httpd.service
systemctl enable httpd.service
  USER_DATA

  lifecycle {
    create_before_destroy = true
  }
}



#alb-autoscaling-GP.tf


resource "aws_autoscaling_group" "east-web" {
  name = "${aws_launch_configuration.east-web.name}-asg"

  min_size             = 1
  desired_capacity     = 2
  max_size             = 3

  health_check_type    = "ELB"
  #load_balancers= ["${aws_alb.alb.id}" ] #classic
  target_group_arns   = ["${aws_alb_target_group.hholic-east-frontend.arn}"]
  #alb = "${aws_alb.alb.id}"
  
  launch_configuration = "${aws_launch_configuration.east-web.name}"
  ####  availability_zones = ["ap-southeast-1a", "ap-southeast-1b"]  아ㅐㄹ vpc_zone_identifier 와 중복

  enabled_metrics = [
    "GroupMinSize",
    "GroupMaxSize",
    "GroupDesiredCapacity",
    "GroupInServiceInstances",
    "GroupTotalInstances"
  ]

  metrics_granularity="1Minute"

  vpc_zone_identifier  = [
       "${aws_subnet.east-1-public_1a.id}",
       "${aws_subnet.east-1-public_1c.id}"
  ]

  # Required to redeploy without an outage.
  lifecycle {
    create_before_destroy = true
  }

  tag {
    key                 = "Name"
    value               = "east-web-autoscaling"
    propagate_at_launch = true
  }
}

resource "aws_autoscaling_attachment" "east-asg-attachment" {
  autoscaling_group_name = aws_autoscaling_group.east-web.id
  alb_target_group_arn   = aws_alb_target_group.hholic-east-frontend.arn
}


#autoscaling-policy.tf
#UP
resource "aws_autoscaling_policy" "east-web_policy_up" {
  name = "east-web_policy_up"
  scaling_adjustment = 1
  adjustment_type = "ChangeInCapacity"
  cooldown = 10
  autoscaling_group_name = "${aws_autoscaling_group.web-05.name}"
}

resource "aws_cloudwatch_metric_alarm" "web_cpu_alarm_up" {
  alarm_name = "east-web_cpu_alarm_up"
  comparison_operator = "GreaterThanOrEqualToThreshold"
  evaluation_periods = "2"
  metric_name = "CPUUtilization"
  namespace = "AWS/EC2"
  period = "120"
  statistic = "Average"
  threshold = "20"

  #dimensions {
  #  AutoScalingGroupName = "${aws_autoscaling_group.web.name}"
  #}

  alarm_description = "This metric monitor EC2 instance CPU utilization"
  alarm_actions = ["${aws_autoscaling_policy.east-web_policy_up.arn}"]
}

#Down
resource "aws_autoscaling_policy" "east-web_policy_down" {
  name = "east-web_policy_down"
  scaling_adjustment = -1
  adjustment_type = "ChangeInCapacity"
  cooldown = 10
  autoscaling_group_name = "${aws_autoscaling_group.east-web.name}"
}

resource "aws_cloudwatch_metric_alarm" "web_cpu_alarm_down" {
  alarm_name = "east-web_cpu_alarm_down"
  comparison_operator = "LessThanOrEqualToThreshold"
  evaluation_periods = "2"
  metric_name = "CPUUtilization"
  namespace = "AWS/EC2"
  period = "120"
  statistic = "Average"
  threshold = "10"

  #dimensions {
  #  AutoScalingGroupName = "${aws_autoscaling_group.east-web.name}"
  #:}

  alarm_description = "This metric monitor EC2 instance CPU utilization"
  alarm_actions = ["${aws_autoscaling_policy.east-web_policy_down.arn}"]
}





   1  git
    2  git init
    3  git pull
    4  git clone https://github.com/minsookim78/AWS-Team-PJ.git
    5  ls
    6  wget https://releases.hashicorp.com/terraform/0.12.26/terraform_0.12.26_linux_amd64.zip
    7  unzip terraform_0.12.26_linux_amd64.zip 
    8  cd ~/.ssh/
    9  ls
   10  cat authorized_keys 
   11  ssh-keygen -t rsa
   12  ls
   13  cat id_rsa.pub 
   14  nano authorized_keys 
   15  cp id_rsa.pub Hholic.pem
   16  cat Hholic.pem 
   17  cd ~/environment/
   18  ./terraform plan
   
   27  ./terraform plan
   28  cd ~/.ssh/
   29  ls
   30  chmod 400 Hholic.pem
   31  ssh -i "Hholic.pem" ec2-user@ec2-52-90-21-51.compute-1.amazonaws.com
   32  cat Hholic.pem 
   33  ls
   34  history
   35  cat id_rsa
   36  ls
   37  rm Hholic.pem 
   38  ls
   39  cp id_rsa Hholic.pem
   40  chmod 400 Hholic.pem
   41  ssh -i "Hholic.pem" ec2-user@ec2-3-230-147-141.compute-1.amazonaws.com
   42  ls
   43  cat authorized_keys 
   44  cat known_hosts 
   45  nano known_hosts 
   46  ssh -i "Hholic.pem" ec2-user@ec2-3-230-147-141.compute-1.amazonaws.com
   47  ls
   48  rm Hholic.pem 
   49  cp id_rsa.pub Hholic.pem
   50  ssh -i "Hholic.pem" ec2-user@ec2-3-230-147-141.compute-1.amazonaws.com
   51  chmod 400 Hholic.pem
   52  ssh -i "Hholic.pem" ec2-user@ec2-3-230-147-141.compute-1.amazonaws.com
   53  ssh-keygen -f Hholic -m pem -p
   54  ssh-keygen -f  -m pem -p
   55  ssh-keygen -f id_rsa.pub -m pem -p
   56  openssl rsa -in id_rsa -pubout -out id_rsa.pub.pe
   57  openssl rsa -in id_rsa -pubout -out id_rsa.pub.pem
   58  ls
   59  cat id_rsa.pub.pem 
   60  cat id_rsa.pub
   61  cp id_rsa.pub Hholic.pub
   62  cp id_rsa.pub.pem Hholic.pem
   63  ls
   64  rm Hholic.pem 
   65  cp id_rsa.pub.pem Hholic.pem
   66  ls
   67  ssh -i "Hholic.pem" ec2-user@ec2-3-230-147-141.compute-1.amazonaws.com
   68  chmod 440 Hholic.pem 
   69  ssh -i "Hholic.pem" ec2-user@ec2-3-230-147-141.compute-1.amazonaws.com
   70  chmod 400 Hholic.pem 
   71  ssh -i "Hholic.pem" ec2-user@ec2-3-230-147-141.compute-1.amazonaws.com
   72  history
   73  ls
   74  rm Hholic.pem Hholic.pub id_rsa id_rsa.pub id_rsa.pub.pe id_rsa.pub.pem
   75  ls
   76  ssh-keygen 
   77  ls
   78  cat id_rsa.pub 
   79  nano authorized_keys 
   80  cp id_rsa Hholic.pem
   81  cp id_rsa.pub Hholic.pub
   82  cat Hholic.pub 
   83  chmod 400 Hholic.pem 
   84  ssh -i "Hholic.pem" ec2-user@ec2-3-90-2-99.compute-1.amazonaws.com
   85  ssh -i "Hholic.pem" ec2-user@ec2-54-157-97-89.compute-1.amazonaws.com
   86  git
   87  git init
   88  ls
   89  git add Hholic.pem 
   90  git add Hholic.pub 
   91  git commit -m "v1 0"
   92  git pull https://github.com/minsookim78/AWS-Team-PJ.git
   93  git log
   94  git push origin master
   95  git status
   96  git remote add origin master
   97  git push origin master
   98  git push master https://github.com/minsookim78/AWS-Team-PJ.git
   99  git remote add origin https://github.com/minsookim78/AWS-Team-PJ.git
  100  git push origin master
  101  git init
  102  git log
  103  git status
  104  git remote add origin https://github.com/minsookim78/AWS-Team-PJ.git
  105  git remote add https://github.com/minsookim78/AWS-Team-PJ.git
  106  git remote -v
  107  git clone https://github.com/minsookim78/AWS-Team-PJ.git
  108  git remote -v
  109  ls
  110  git push origin master
  111  git remote -v
  112  git remote add https://github.com/minsookim78/AWS-Team-PJ.git
  113  git remote add origin https://github.com/minsookim78/AWS-Team-PJ.git
  114  git remote delete origin master
  115  history



data "aws_s3_bucket" "selected" {
  bucket = "a-test-bucket"
}

resource "aws_cloudfront_distribution" "test" {
  origin {
    domain_name = "${data.aws_s3_bucket.selected.bucket_domain_name}"
    origin_id   = "s3-selected-bucket"
  }
}

git init : git 생성하기
git clone git_path : 코드가져오기
git checkout branch_name : 브랜치 선택하기
git checkout -t remote_path/branch_name : 원격 브랜치 선택하기
git branch branch_name : 브랜치 생성하기
git branch -r : 원격 브랜치 목록보기
git branch -a : 로컬 브랜치 목록보기
git branch -m branch_name change_branch_name : 브랜치 이름 바꾸기
git branch -d branch_name : 브랜치 삭제하기
git push remote_name — delete branch_name : 원격 브랜치 삭제하기 ( git push origin — delete gh-pages )
git add file_path : 수정한 코드 선택하기 ( git add * )
git commit -m “commit_description” : 선택한 코드 설명 적기 ( git commit -m “내용”)
git push romote_name branch_name : add하고 commit한 코드 git server에 보내기 (git push origin master)
git pull : git서버에서 최신 코드 받아와 merge 하기
git fetch : git서버에서 최신 코드 받아오기
git reset — hard HEAD^ : commit한 이전 코드 취소하기
git reset — soft HEAD^ : 코드는 살리고 commit만 취소하기
git reset — merge : merge 취소하기
git reset — hard HEAD && git pull : git 코드 강제로 모두 받아오기
git config — global user.name “user_name ” : git 계정Name 변경하기
git config — global user.email “user_email” : git 계정Mail변경하기
git stash / git stash save “description” : 작업코드 임시저장하고 브랜치 바꾸기
git stash pop : 마지막으로 임시저장한 작업코드 가져오기
git branch — set-upstream-to=remote_path/branch_name : git pull no tracking info 에러해결
 
 Github에서 angular를 받아오고 싶다면, 아래 명령어로 받아 올 수 있습니다.
 git pull // 코드를 받아와 변경점을 merge한다.
git fetch // 코드를 받아온다.
git clone https://github.com/angular/angular.git

Commit 취소하기
git은 코드의 상태를 uncommit / staying / commit 으로 나눠 볼수 있습니다.
코드 파일의 내용을 바꾸면 uncommit이 됩니다.
파일의 변경 내역 add 하면 index에 추가되고 staying이 됩니다.
변경 내용을 commit으로 확정지을 수 있씁니다.
commit 이 완료되면 서버에 push할 수 있게 됩니다.
코드를 작성하다보면 잘못된 코드를 추가하거나 merge 할 수 있습니다. 이를 취소하고싶다면, git reset 으로 취소 할 수 있습니다.
git reset // add 한 파일 취소
git reset — -merge // merge 한 코드 취소
만약 commit한 코드를 취소하고 이전코드로 돌아가고 싶다면, git reset HEAD^ 명령어를 이용하여 바로 직전에 commit한 코드로 되돌아 갈 있습니다.
하지만 이경우 작성하고 수정했던 코드는 모두 사라지게 됩니다. 만약 commit만 취소하고, 작성한 코드는 살리고 싶다면 어떻게 해야할까요? reset 명령어의 옵션중에 soft를 이용하면 됩니다.
git reset — soft HEAD^ //commit 코드 살리기
git reset — hard HEAD^ //commit하기 이전의 코드로 돌아가기
git reset을 이용하여 작성한 코드를 초기화하고 서버로부터 다시 받아 올 수 도 있습니다.
git reset — hard HEAD // HEAD를 기준으로 이전코드로 돌아갑니다.
git pull // git 서버에서 코드를 다시 받아옵니다.
코드 잠깐 저장하고, 다른 브랜치로 이동하기
A라는 Branch에서 작업을 하던중 급하게 들어온 요청 혹은 Bug가있어 B 브랜치로 이동해야할 일이 생겼습니다. 그런데 작업은 완료되지 않아서 commit 해두기에는 애매하고, 그렇다고 다시 작업하기에는 작업량이 있어 버릴수는 없는경우에 어떻게 해야할까요?
그럴 경우 잠깐 다른 공간에 변경 내역을 기록해 두고 브랜치를 변경해서 작업한후 다시 돌아와서 변경 내역을 불러와 계속 작업을 이어 나갈 수 있습니다.
git stash // 코드를 stash 영역에 잠시 저장하기
stash 명령어를 이용하여 stash 영역에 코드를 잠깐 저장해두고 다시 돌아와 작업을 이어나갈수 있게 되었습니다. 해당 구조는 스택구조로 이루어져있기때문에 LIFO으로 작업을 불러올수 있습니다.
git stash pop // 마지막에 저장한 코드 불러오기
stash 영역이 기억이 나지 않는다면 git stash list로 목록도 확인해 볼수 있습니다.
